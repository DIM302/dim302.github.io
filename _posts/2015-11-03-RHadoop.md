---
title: "RHadoop Word Count"
layout: post
comments: true
category: R
---

{% raw %}

# Guide to Runnng RHadoop over Amazon EMR #

###  Assumptions ###

That you have already set up an [Amazon AWS](http://aws.amazon.com/) account. <br>
That you have basic knowledge of the following: <br>
- [Amazon EMR (Elastic MapReduce)](http://aws.amazon.com/elasticmapreduce/) <br>
- [Amazon S3 (Simple Storage)](http://aws.amazon.com/s3/) <br>
- [Amazon EC2 (Elastic Compute Cloud)](http://aws.amazon.com/ec2/) <br>
- [R Programming](https://www.r-project.org/) <br>

### Setting up Elastic Map Reduce ###

Step 1: Log into [Amazon AWS](https://aws.amazon.com/)

Step 2: Click the EMR icon - Managed Hadoop Framework <br><br>
![plot of chunk image1](/figure/2015-10-15-RHadoop/image1.png)

Step 3: Create your cluster <br>
- Click "Create cluster" and select "Go to advanced options" at the top of the page <br>
- Give your cluster a name <br>
- Log folder S3 location: Select any location in S3 for logs <br>
- Software configuration: This demo is done using Amazon Hadoop 2 version 3.10.0 <br>
- Add any applications you are interested in using, such as Spark or Mahout <br>
- Choose an EC2 instance: This demo is done using m1.xlarge (Master & Core) and m1.medium (Task) <br>
- Choose your EC2 key pair <br><br>
![plot of chunk image2](/figure/2015-10-15-RHadoop/image2.png) <br>

<b> IMPORTANT BOOTSTRAP INSTRUCTION </b><br>

1. Download the [Amazon EMR Boostrap File](https://docs.google.com/uc?authuser=0&id=0B_DFy-IMDAf4aENDYXdVeGhOV3M&export=download)
2. Open a new tab and upload the .sh script into your S3 (either create a new folder/bucket in S3 or use an existing folder)
3. Within EMR, select "custom action" from the "Select a boostrap action" drop down.  Then select "Configure and add"
4. Link the above .sh file in "S3 location" and press "Add" <br><br>
![plot of chunk image3](/figure/2015-10-15-RHadoop/image3.png)
-  Select any other fields as you wish and click on the "Create cluster" button

IMPORTANT NOTE: <br>
The cluster will initially show as "pending".  With the bootstrap script, the cluster will take around 15 to fully complete the setup process.

Initial Stage: <br>
- All steps should be "Pending"

Second Stage: <br>
- Progress should be shown as "Bootstrapping"

Final Stage: <br>
- Cluster will be shown as "Running"
- Your EMR cluster is now ready to be used with RHadoop.


<b> Running R over the Cluster </b><br>

Open the Terminal and connect to your cluster with the following code

```r
ssh -i /path/to/keypair.pem hadoop@Amazon_Public_DNS
```

Once connected, either make a directory for your input file or download directly into the /home/hadoop directory.  You can imort data from Amazon S3 with the wget command.  The file link can be found under 'Properties' within S3 and you will need to make sure that the correct permissions are set for the file to be downloaded.

```r
mkdir shakespeare
cd shakespeare
wget https://s3-us-west-2.amazonaws.com/bucket-name/midsummer.txt

# Another option by downloading the file from a url
shakespeare <- "data/shakespeare_works.txt"
if(!file.exists(shakespeare)) {
  download.file(url = "http://www.gutenberg.org/ebooks/100.txt.utf-8",
                destfile = shakespeare)
}
```

<b> Starting R </b>

The following code is an example of running a word count on a text file using the rmr2 package and loading data into HDFS with the rhdfs package.

```r
sudo R

Sys.setenv(HADOOP_HOME="/home/hadoop");
Sys.setenv(HADOOP_CMD="/home/hadoop/bin/hadoop");
Sys.setenv(JAVA_HOME="/usr/lib/jvm/java") ;
Sys.setenv(HADOOP_PREFIX="/home/hadoop/");
Sys.setenv(HADOOP_CMD="/home/hadoop/bin/hadoop");
Sys.setenv(HADOOP_STREAMING="/home/hadoop/contrib/streaming/hadoop-streaming.jar");

library(rmr2)
library(rhdfs)
hdfs.init()

# Copy the file to HDFS
hdfs.mkdir("/user/shakespeare/wordcount/data")
hdfs.put("/home/hadoop/shakespeare/shakespeare.txt", "/user/shakespeare/wordcount/data")


map = function(., lines) { 
	keyval(
	unlist(
  strsplit(
	x = lines,
  split = " +")),
1)}

reduce = function(word, counts) { 
	keyval(word, sum(counts))
	}
	
wordcount = function (input, output=NULL) {
mapreduce(input=input, output=output, input.format="text",
       map=map, reduce=reduce)
}	

hdfs.root <- '/user/shakespeare/wordcount'
hdfs.data <- file.path(hdfs.root, 'data')
hdfs.out <- file.path(hdfs.root, 'out')

system.time(out <- wordcount(hdfs.data, hdfs.out6))

results = from.dfs(out)
results.df = as.data.frame(results, stringsAsFactors=F )
results.df[order(-results.df$val, results.df$key)[1:20],]
```

<b>Example of output: </b><br>

![plot of chunk image4](/figure/2015-10-15-RHadoop/image4.png) <br>

<b> REMEMBER TO STOP OR TERMINATE YOUR INSTANCE </b><br>

{% endraw %}

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-57468410-2', 'auto');
  ga('send', 'pageview');

</script>
